import tensorflow as tf
import tensorflow.contrib.seq2seq as seq2seq
import os 
import csv
import numpy as np
import math
from tensorflow.python.ops.rnn_cell import LSTMCell
from tensorflow.python.ops.rnn_cell import MultiRNNCell

from tensorflow.contrib.seq2seq.python.ops import attention_wrapper
from tensorflow.python.ops import embedding_ops

embedding_size = 32
sour_voc_size = 1000
targ_voc_size = 1000

batch_size = 32
hidden_size = 12
source_voc= open('vocab.50K.en.txt', encoding = 'utf-8')
target_voc = open('vocab.50K.de.txt', encoding = 'utf-8')
encoder_inputs  = []
decoder_inputs = []
#embedding
encoder_inputs = tf.placeholder(shape=(None, None), dtype = tf.int32, name = 'encoder_inputs')
decoder_inputs = tf.placeholder(shape=(None, None), dtype = tf.int32, name = 'decoder_inputs')
embedding_encoder = tf.Variable(tf.random_uniform([sour_voc_size, embedding_size], -1.0,
                                                  1.0), dtype = tf.float32)

#before I used:embedding_encoder = tf.Variable('embedding_encoder', [sour_voc_size, embedding_size])

encoder_emb_imp = tf.nn.embedding_lookup(embedding_encoder, encoder_inputs)

embedding_decoder = tf.Variable(tf.random_uniform([targ_voc_size, embedding_size], -1.0,
                                                  1.0), dtype = tf.float32)
decoder_emb_imp = tf.nn.embedding_lookup(embedding_decoder, decoder_inputs)

'''

embedding & decoder initialization
def _init_(self, hidden_size, input_size, batch_size, num_layers = 2, bidirectonal = True):
    super(EncoderRNN, self)._init_()
    self.batch_size = batch_size
    self.num_layers = num_layers
    self.bidirectional = bidirectional
    self.hidden_size = hidden_size
    
    self.embedding = nn.Embedding(input_size,embedding_dim = hidden_size)
    
    self.lstm = nn.LSTM(input_size = hidden_size, hidden_size = hidden_size, num_layers = num_layers)
'''

#encoder rnn cell
num_units = 128
encoder_cell = tf.nn.rnn_cell.LSTMCell(num_units = num_units)

#it uses bidirectional RNN, so the code should be written  this way:
sequence_length = 128
def bidirectional_dynamic_rnn(cell_fw, cell_bw, inputs, sequence_length, initial_state_fw = None,
                             initial_state_bw = None, dtype = None, parallel_iterations = None, 
                             swap_memory = False, time_major =True, scope = None)
#build decoder rnn cell
decoder_cell = tf.nn.rnn_cell.LSTMCell(num_units = num_units)

#Helper
helper = tf.contrib.seq2seq.TrainingHelper(decoder_emb_inp, decoder_lengths, time_major = True)
decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell, helper, encoder_state, output_layer = projection_layer)
#Dynamic decoding
outputs, _ = tf.contrib.seq2seq.dynamic_decode(decoder)â€‹#Helper
helper = tf.contrib.seq2seq.TrainingHelper(decoder_emb_inp, decoder_lengths, time_major = True)
decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell, helper, encoder_state, output_layer = projection_layer)
#Dynamic decoding
outputs, _ = tf.contrib.seq2seq.dynamic_decode(decoder)
logits = outputs.rnn_output
projection_layer = layers_core.Dense(target_voc_size, use_bias = False)





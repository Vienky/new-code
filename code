import tensorflow as tf
import tensorflow.contrib.seq2seq as seq2seq
import os 
import csv
import numpy as np
import math
from tensorflow.python.ops.rnn_cell import LSTMCell
from tensorflow.python.ops.rnn_cell import MultiRNNCell
from tensorflow.python.ops.rnn_cell import RNNCell

from tensorflow.contrib.seq2seq.python.ops import attention_wrapper
from tensorflow.python.ops import embedding_ops
from tensorflow.python.util import nest

tf.reset_default_graph()

embedding_size = 32
sour_voc_size = 1000
targ_voc_size = 1000
max_time = 10
decoder_lengths = 20
max_sentence_length = 30
learning_rate = 0.8
beam_size = 10
num_units = 128
batch_size = 80
hidden_size = 12
with open('vocab.50K.en.txt', encoding = 'utf-8') as f:
    source_voc = str(f)
with open('vocab.50K.de.txt', encoding = 'utf-8') as f:
    target_voc = str(f)
word_to_idx = []
mode = {}
encoder_inputs  = []
decoder_inputs = []
#define encoder&decoder inputs
encoder_inputs = tf.placeholder(shape=(batch_size, max_time), dtype = tf.int32, name = 'encoder_inputs')
decoder_inputs = tf.placeholder(shape=(batch_size, max_time), dtype = tf.int32, name = 'decoder_inputs')
encoder_inputs_length = tf.placeholder(tf.int32, [batch_size], name = 'encoder_inputs_length')
decoder_inputs_length = tf.placeholder(tf.int32, [batch_size], name = 'decoder_inputs_length')
#embedding 
embedding_encoder = tf.Variable(tf.random_uniform([sour_voc_size, embedding_size], -1.0,
                                                  1.0), dtype = tf.float32)

'''
embedding_encoder = tf.Variable('embedding_encoder', [sour_voc_size, embedding_size])

'''

encoder_emb_imp = tf.nn.embedding_lookup(embedding_encoder, encoder_inputs)

embedding_decoder = tf.Variable(tf.random_uniform([targ_voc_size, embedding_size], -1.0,
                                                  1.0), dtype = tf.float32)
decoder_emb_imp = tf.nn.embedding_lookup(embedding_decoder, decoder_inputs)
max_target_sequence_length = tf.reduce_max(decoder_inputs_length, name='max_target_len')

#text process
def extract_character_voc(voc):
    
    special_words = ['<PAD>', '<UNK>', '<GO>', '<EOS>']
    
    set_words = list(set([character for line in source_voc.split('\n') for character in line]))
    # put these special words into the dictionary
    int_to_voc = {idx: word for idx, word in enumerate(special_words + set_words)}
    voc_to_int = {word: idx for idx, word in int_to_voc.items()}
    
    return int_to_voc, voc_to_int
    
    source_int_to_letter, source_letter_to_int = extract_character_voc(source_voc)
target_int_to_letter, target_letter_to_int = extract_character_voc(target_voc)

source_int = [[source_letter_to_int.get(letter, source_letter_to_int['<UNK>']) for letter in line]
                                        for line in source_voc.split('\n')]
target_int = [[target_letter_to_int.get(letter, target_letter_to_int['<UNK>']) for letter in line] 
                                        + [target_letter_to_int['<EOS>']] for line in target_voc.split('\n')]
                                        
#encoder rnn cell
'''
encoder_cell = tf.nn.rnn_cell.LSTMCell(num_units = num_units)
encoder_outputs, encoder_state = tf.nn.dynamic_rnn(encoder_cell, encoder_emb_imp, 
                                                   sequence_length = encoder_inputs_length, dtype = tf.float32)'''
#it uses bidirectional RNN, so the code could be written in this way:


lstm_fw_cell = tf.nn.rnn_cell.LSTMCell(hidden_size)
lstm_fw_cwll = tf.nn.rnn_cell.DropoutWrapper(cell = lstm_fw_cell)
lstm_bw_cell = tf.nn.rnn_cell.LSTMCell(hidden_size)
lstm_bw_cell = tf.nn.rnn_cell.DropoutWrapper(cell = lstm_bw_cell)

output, enc_state = tf.nn.bidirectional_dynamic_rnn(lstm_fw_cell, lstm_bw_cell, encoder_emb_imp, sequence_length=encoder_inputs_length, dtype = tf.float32)

output = tf.concat([output[0], output[1]], 2)
enc_hidden_state  = tf.concat([enc_state[1][0], enc_state[1][1]], 1)
enc_hidden_state = tf.expand_dims(enc_hidden_state, 2)


mask = tf.sequence_mask(decoder_inputs_length, max_target_sequence_length, dtype = tf.float32, name = 'masks')
with tf.variable_scope('decoder'):
    '''encoder_outputs = tf.contrib.seq2seq.tile_batch(encoder_outputs, multiplier = beam_size)
    encoder_state = nest.map_structure(lambda s: tf.contrib.seq2seq.tile_batch(s, beam_size), encoder_state)
    encoder_inputs_length = tf.contrib.seq2seq.tile_batch(encoder_inputs_length, multiplier = beam_size)'''
    encoder_inputs_length = encoder_inputs_length
    attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(num_units = num_units, memory = encoder_outputs,
                                                              memory_sequence_length = encoder_inputs_length)
    #decoder rnn cell & attention Wrapper
    
    decoder_cell = tf.nn.rnn_cell.LSTMCell(num_units = num_units)
    decoder_cell = tf.contrib.seq2seq.AttentionWrapper(cell = decoder_cell, attention_mechanism = attention_mechanism,
                                                      attention_layer_size = num_units, name = 'Attention_Wrapper')
    
    batch_size = batch_size
    #initialize the decoder, use encoder(ht)
    decoder_initial_state = decoder_cell.zero_state(batch_size = batch_size, dtype = tf.float32).clone(cell_state = encoder_state)
    projection_layer = tf.layers.Dense(target_voc_size, kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev = 0.1))
    
    if mode == 'train':
        #define the inputs of the decoder, add beginning character and delete the ending character
        ending = tf.strided_slice(decoder_inputs, [0, 0], [batch_size, -1], [1, 1])
        decoder_input = tf.concat([tf.fill([batch_size, 1], word_to_idx['<GO>']), ending], 1)
        decoder_inputs_embedding = tf.nn.embedding_lookup(embedding, decoder_input)
        
        Training_helper = tf.contrib.seq2seq.TrainingHelper(inputs = decoder_inputs_embedding, 
                                                    sequence_length = encoder_inputs_length, time_major = False,
                                                    name = 'Training_helper')
        traing_decoder = tf.contrib.seq2seq.BasicDecoder(cell = decoder_cell, help = training_helper,
                                                         initial_state = decoder_initial_state, output_layer = projection_layer)
        decoder_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder = training_decoder, impute_finished = True,
                                                                 maximum_iterations = self.max_target_sequence_length)

